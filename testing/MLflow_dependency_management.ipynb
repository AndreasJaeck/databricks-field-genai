{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a80913d4-4405-496d-9107-6dfb303beea5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Different python versions\n",
    "\n",
    "This notebook demonstrates what happens when you have a model trained on a different version of python, and try to run inference in a databricks notebook with a different python version.\n",
    "\n",
    "We have user story [SUP-12708](https://allianzdirect.atlassian.net/browse/SUP-12708) open for the Allianz Direct Data Team to address this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c01e59d0-1b7f-437c-94dd-6b264ff14f4e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!python -V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "631afbce-727f-4b1f-99ec-6295cc59378b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Note that we have already trained an model, using python 3.11, and registered the model to MLflow. For clarify, here is the training code used (not run in this notebook.).\n",
    "\n",
    "Train model: \n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.datasets import make_hastie_10_2\n",
    "\n",
    "# Data and training\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "X = pd.DataFrame(X, columns=[f\"f{i}\" for i in range(X.shape[1])])\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]\n",
    "clf = HistGradientBoostingClassifier(max_iter=100).fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "Log model:\n",
    "\n",
    "```python\n",
    "import mlflow\n",
    "from ds_workmate.mlflow import setup_mlflow_env\n",
    "\n",
    "# Login to MLflow\n",
    "setup_mlflow_env()\n",
    "client = mlflow.MlflowClient(registry_uri=\"databricks\")\n",
    "experiment = client.get_experiment_by_name(\"/machine-learning-models/ds_ramen\")\n",
    "\n",
    "with mlflow.start_run(experiment_id=experiment.experiment_id):\n",
    "    mlflow.sklearn.log_model(\n",
    "            sk_model=clf,\n",
    "            artifact_path=\"model\",\n",
    "            code_paths=None,\n",
    "            signature=None,  # inferred from input_example\n",
    "            input_example=X_train.sample(1),\n",
    "            pyfunc_predict_fn=\"predict_proba\",\n",
    "            pip_requirements=[\"scikit-learn==1.5.0\"],\n",
    "            registered_model_name=\"test_tim\",\n",
    "    )\n",
    "```\n",
    "\n",
    "We can load this model back and do inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "605e3ec0-2de8-4234-8ef5-26b917028a9d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Install mlflow so we can interact with it\n",
    "!pip install mlflow\n",
    "\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfbe162e-21b4-43a8-98cb-199044d6a83b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate some sample inference data\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_hastie_10_2\n",
    "X, y = make_hastie_10_2(n_samples=5, random_state=0)\n",
    "X = pd.DataFrame(X, columns=[f\"f{i}\" for i in range(X.shape[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1130cbc-7b62-4d1e-874f-8bc35fa31003",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "mlflow.set_registry_uri(\"databricks\")\n",
    "\n",
    "mlflow.models.predict(model_uri=\"models:/test_tim/12\", input_data=X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c859b1db-6116-4f83-8d80-71dad31ba274",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pyenv install --list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "510f610f-9c01-41c4-8730-9e090c2c181d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16c7085d-2e8a-43b8-8f5f-6fdde6641f1e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Library version management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9856f56-ea33-485d-a207-e72e7bfb60da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!python -V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b493d3e-21bb-4818-bae0-1e34f19e8755",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28e6aeb7-1b9c-4049-81e1-734e407463f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# For demonstration purpose we install a scikit learn version that is different to the training env \n",
    "%pip install \"scikit-learn==1.5.0\"\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4637b1e0-b3e8-4c2f-954a-43ec30ec997f",
     "showTitle": true,
     "title": "Disable MLFlow Autolog"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# Disable MLflow autolog so we don't get unnecessary experiment runs\n",
    "mlflow.autolog(disable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78390ba8-814e-45ba-9e81-5d7c063fffcb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.datasets import make_hastie_10_2\n",
    "\n",
    "# Data and training\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "X = pd.DataFrame(X, columns=[f\"f{i}\" for i in range(X.shape[1])])\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "140455e8-0ca7-4c5c-88b0-70cc0b428a3d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "\n",
    "# Login to MLflow\n",
    "#setup_mlflow_env()\n",
    "#client = mlflow.MlflowClient(registry_uri=\"databricks\")\n",
    "#experiment = client.get_experiment_by_name(\"/machine-learning-models/ds_ramen\")\n",
    "\n",
    "catalog = \"dbdemos_aj\"\n",
    "schema = \"test_schema\"\n",
    "model_name = \"test_model\"\n",
    "\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "with mlflow.start_run() as r:\n",
    "\n",
    "    # run training \n",
    "    clf = HistGradientBoostingClassifier(max_iter=100).fit(X_train, y_train)\n",
    "\n",
    "    # log metrics, parameters, schema\n",
    "\n",
    "\n",
    "    # log model \n",
    "    # AJ: please avoid specifing a ml framework version in pip requirements parameter that is different from the version the model was trained with. Mlflow will automatically log the version the model was trained with. If you need a framework version that is different from env version %pip install requirements.txt before doing the training run. \n",
    "    mlflow.sklearn.log_model(\n",
    "            sk_model=clf,\n",
    "            artifact_path=\"model\",\n",
    "            code_paths=None,\n",
    "            signature=None,  # inferred from input_example\n",
    "            input_example=X_train.sample(1),\n",
    "            pyfunc_predict_fn=\"predict_proba\",\n",
    "            registered_model_name=f\"{catalog}.{schema}.{model_name}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98bf0ac0-0d20-4291-bcd1-f1b661b33774",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Inference \n",
    "\n",
    "Inference environments are often different from the training env. Mlflow allows you to either bring the dependencies to the inference envionment (Option 1) or generate generic Python and Spark functions from model that no longer require ml framework dependencies in the inference envionment (Option 2). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30290aaf-7458-46a9-a8e2-e1e9629f02ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Install scikit learn version that os different to the training env\n",
    "%pip install \"scikit-learn==1.3.0\"\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ad3ee74-015b-4c9b-8672-b2bec166f6d9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Option 1: Use mlflow model flavour + install dependencies \n",
    "With get_model_dependencies we can download the requirements.txt of the model to the inference env. Install to the inference envionment and run the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "914a81ca-6207-4759-87a6-d50cb39a8b1e",
     "showTitle": true,
     "title": "Create model config"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow \n",
    "import yaml\n",
    "\n",
    "catalog = \"dbdemos_aj\"\n",
    "schema = \"test_schema\"\n",
    "model_name = \"test_model\"\n",
    "model_version = 3\n",
    "\n",
    "config = {\n",
    "    \"model_resources\": {\n",
    "        \"catalog\": catalog,\n",
    "        \"schema\": schema,\n",
    "        \"model_name\": model_name,\n",
    "        \"model_version\": model_version,\n",
    "        \"model_uri\": f\"models:/{catalog}.{schema}.{model_name}/{model_version}\"\n",
    "    },\n",
    "}\n",
    "\n",
    "try:\n",
    "    with open(\"config.yaml\", \"w\") as f:\n",
    "        yaml.dump(config, f)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to write YAML file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90379cfc-32ca-4c80-af84-abea22cd4296",
     "showTitle": true,
     "title": "Install model dependencies"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import yaml\n",
    "\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Get requirements.txt from model \n",
    "model_dependencies = mlflow.pyfunc.get_model_dependencies(config[\"model_resources\"][\"model_uri\"])\n",
    "\n",
    "# Install dependencies\n",
    "%env MODEL_DEPENDENCIES=$model_dependencies\n",
    "%pip install -r $MODEL_DEPENDENCIES\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "229fad7d-a51a-4e81-97e7-4831b7fb8c0e",
     "showTitle": true,
     "title": "Load model"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import yaml\n",
    "\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# AJ: Make sure to use the right model flavour when loading the model. \n",
    "sklearn_model = mlflow.sklearn.load_model(model_uri=config[\"model_resources\"][\"model_uri\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81d5b73c-7998-47d1-8948-97587c11909e",
     "showTitle": true,
     "title": "Run predict"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_hastie_10_2\n",
    "import pandas as pd\n",
    "\n",
    "# Data and training\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "X = pd.DataFrame(X, columns=[f\"f{i}\" for i in range(X.shape[1])])\n",
    "\n",
    "# Use sklearn model to predict\n",
    "sklearn_model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16e182c4-7ec4-439e-b32c-53aff16450ff",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Option 2: Use generic mlflow pyfunc flavour\n",
    "\n",
    "Mlflow can convert models into a generic python function - even when model was trained with a different model flavour. This will allow you to use the model in the inference env wihout any dependency to the training env. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f68c8c2e-ae3a-4da7-a49c-7750d6e11727",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import yaml\n",
    "\n",
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "pyfunc_model = mlflow.pyfunc.load_model(model_uri=config[\"model_resources\"][\"model_uri\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e69f933-4040-4dfe-9801-fd2062fc1e7a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Data and training\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "X = pd.DataFrame(X, columns=[f\"f{i}\" for i in range(X.shape[1])])\n",
    "\n",
    "# Use sklearn model to predict\n",
    "pyfunc_model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d3f1116-8393-44a3-a960-afece5890f67",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Pyenv management\n",
    "\n",
    "We recommend to manage library version's instead of entire python envionments (see chapter on top) and upgrade the DBR runtime if necessary. Below is an example where we use a DBR runtime with an older version of Python for inference. The inference will work properly, as long we can install the right version of sklearn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d359614-7561-4038-bde3-e984a8317b75",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!python -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c4b35d3-2008-492b-aba2-7e92fbbc6d97",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install mlflow --upgrade\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64bef6dd-4656-441c-bb70-6a502460ad62",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import yaml\n",
    "\n",
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "# Get requirements.txt from model \n",
    "model_dependencies = mlflow.pyfunc.get_model_dependencies(model_uri=config[\"model_resources\"][\"model_uri\"])\n",
    "\n",
    "# Install dependencies\n",
    "%env MODEL_DEPENDENCIES=$model_dependencies\n",
    "%pip install -r $MODEL_DEPENDENCIES\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc6f7e67-2f6a-40b1-b57d-b421263342c6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import yaml\n",
    "\n",
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "sklearn_model = mlflow.sklearn.load_model(model_uri=config[\"model_resources\"][\"model_uri\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8eab85ed-fa2d-4064-aee4-53d0e297fe45",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_hastie_10_2\n",
    "import pandas as pd\n",
    "\n",
    "# Data and training\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "X = pd.DataFrame(X, columns=[f\"f{i}\" for i in range(X.shape[1])])\n",
    "\n",
    "# AJ: The model was trained with a different python version. As long we install the correct library version of sklearn, we can still run the inference. Even if the inference env is running a different python version. To avoid any side effects try to keep training and inference peython versions close together. \n",
    "sklearn_model.predict(X)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "MLflow_dependency_management",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
